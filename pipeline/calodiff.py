# calodiff.py

import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import os
import copy
from typing import Callable, Optional, Dict, List


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (Scheduler'—ã —à—É–º–∞) ---

def _cosine_noise_scheduler(t: torch.Tensor, t_max: int) -> torch.Tensor:
    """
    –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π scheduler –¥–ª—è —É—Ä–æ–≤–Ω—è —à—É–º–∞.
    –ü–ª–∞–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1.
    """
    return 0.5 * (1 - torch.cos(torch.pi * t / t_max))


def _linear_noise_scheduler(t: torch.Tensor, t_max: int) -> torch.Tensor:
    """
    –õ–∏–Ω–µ–π–Ω—ã–π scheduler –¥–ª—è —É—Ä–æ–≤–Ω—è —à—É–º–∞.
    –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1.
    """
    return t / t_max


NOISE_SCHEDULERS = {
    "cosine": _cosine_noise_scheduler,
    "linear": _linear_noise_scheduler
}


# --- –§—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏) ---

def sample(
        model: torch.nn.Module,
        y_conditions: torch.Tensor,
        n_steps: int,
        device: str,
        shape: tuple = (1, 30, 30),
        denoising_scheduler_name: str = "cosine"
) -> torch.Tensor:
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.

    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å.
        y_conditions: –¢–µ–Ω–∑–æ—Ä —Å —É—Å–ª–æ–≤–∏—è–º–∏ (–º–µ—Ç–∫–∞–º–∏) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        n_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cpu' –∏–ª–∏ 'cuda').
        shape: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–±–µ–∑ —É—á–µ—Ç–∞ –±–∞—Ç—á–∞).
        denoising_scheduler_name: –ù–∞–∑–≤–∞–Ω–∏–µ scheduler'–∞ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    Returns:
        –¢–µ–Ω–∑–æ—Ä —Å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.
    """
    n_samples = y_conditions.shape[0]
    x_gen = torch.rand(n_samples, *shape).to(device)  # –ù–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ —à—É–º–∞
    y_conditions = y_conditions.to(device)

    denoising_scheduler = NOISE_SCHEDULERS.get(denoising_scheduler_name)
    if not denoising_scheduler:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π scheduler: {denoising_scheduler_name}")

    model.eval()
    with torch.no_grad():
        for i in tqdm(range(n_steps), desc="Sampling", leave=False):
            t = torch.tensor(i, device=device)
            # –í –≤–∞—à–µ–º –∫–æ–¥–µ timestep –≤—Å–µ–≥–¥–∞ 0, —è —Å–æ—Ö—Ä–∞–Ω–∏–ª —ç—Ç—É –ª–æ–≥–∏–∫—É.
            # –í –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–∫–∞—Ö —Å—é–¥–∞ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —Å–∞–º timestep.
            pred = model(x_gen, 0, y_conditions)

            # –õ–æ–≥–∏–∫–∞ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞
            mix_factor = 1 / (n_steps - i)
            x_gen = x_gen * (1 - mix_factor) + pred * mix_factor

    return x_gen.cpu()


# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è ---

def train(
    model: torch.nn.Module,
    train_loader: DataLoader,
    n_epochs: int,
    loss_fn: Callable,
    optimizer: torch.optim.Optimizer,
    device: str,
    # --- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ---
    valid_loader: Optional[DataLoader] = None,
    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Scheduler'–æ–≤ ---
    lr_scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,
    noise_scheduler_name: str = "cosine",
    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ---
    validation_freq: int = 1,
    n_inference_steps: int = 1000,
    metric_calculator: Optional[object] = None, # –ü–∞—Ä–∞–º–µ—Ç—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    checkpoint_path: str = "./checkpoints",
    # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (Early Stopping) ---
    early_stopping_patience: Optional[int] = None,
    # --- –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ù–ê –¢–ï–°–¢–ï ---
    test_loader: Optional[DataLoader] = None,
    visualize_test_batch: bool = True,
    test_visualization_func: Optional[Callable] = None
) -> Dict[str, List[float]]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–ª—É—á—à–µ–Ω–∏—è train_loss.
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —è–≤–ª—è—é—Ç—Å—è –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏.
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –±–∞—Ç—á–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ.
    """
    if not os.path.exists(checkpoint_path):
        os.makedirs(checkpoint_path)


    noise_scheduler_fn = NOISE_SCHEDULERS.get(noise_scheduler_name)
    if not noise_scheduler_fn:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π scheduler —à—É–º–∞: {noise_scheduler_name}")

    history = {'train_loss': [], 'valid_loss': []}
    
    # --- –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ª—É—á—à–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π ---
    best_train_loss = float('inf')
    best_valid_loss = float('inf')
    best_model_state_on_train = None # –°–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º train_loss
    
    patience_counter = 0

    # –ó–∞—Ä–∞–Ω–µ–µ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –æ–¥–∏–Ω –±–∞—Ç—á –∏–∑ test_loader –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    fixed_test_batch = None
    if test_loader and visualize_test_batch:
        try:
            fixed_test_batch = next(iter(test_loader))
        except StopIteration:
            print("Warning: test_loader –ø—É—Å—Ç, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –±–∞—Ç—á–µ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞.")

    for epoch in range(n_epochs):
        print(f"--- Epoch {epoch + 1}/{n_epochs} ---")
        
        # --- –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è ---
        model.train()
        epoch_train_loss = []
        for x, y in tqdm(train_loader, desc="Training"):
            x, y = x.to(device), y.to(device)
            t = torch.randint(0, n_inference_steps, (x.shape[0],), device=device)
            noise_amount = noise_scheduler_fn(t.float(), n_inference_steps).view(-1, 1, 1, 1).to(device)
            noise = torch.randn_like(x)
            noisy_x = x * (1 - noise_amount) + noise * noise_amount
            pred = model(noisy_x, 0, y)
            loss = loss_fn(x, pred)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_train_loss.append(loss.item())
        
        avg_train_loss = sum(epoch_train_loss) / len(epoch_train_loss)
        history['train_loss'].append(avg_train_loss)
        print(f"Avg Train Loss: {avg_train_loss:.5f}")

        # --- –õ–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø–æ train_loss ---
        # –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if avg_train_loss < best_train_loss:
            best_train_loss = avg_train_loss
            best_model_state_on_train = copy.deepcopy(model.state_dict())
            torch.save(best_model_state_on_train, os.path.join(checkpoint_path, "best_model_on_train.pth"))
            print(f"üöÄ New best model saved with train loss: {best_train_loss:.5f}")

        # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –±–∞—Ç—á–µ ---
        # –ü—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ, –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
        if visualize_test_batch and fixed_test_batch is not None and test_visualization_func is not None:
            print("Visualizing examples from the test batch...")
            x_test_real, y_test = fixed_test_batch
            generated_images = sample(
                model, y_test.to(device), n_inference_steps, device,
                shape=(x_test_real.shape[1], x_test_real.shape[2], x_test_real.shape[3])
            )

            n_samples_to_show = min(len(generated_images), 5)
            fig, axs = plt.subplots(1, n_samples_to_show, figsize=(20, 4))
            fig.suptitle(f"Test Batch Visualization at Epoch {epoch + 1}", fontsize=16)

            if n_samples_to_show == 1: axs = [axs] 

            for i, ax in enumerate(axs):
                test_visualization_func(energy=generated_images[i].cpu(), ax=ax)
            plt.show()

        # --- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ---
        if valid_loader and (epoch + 1) % validation_freq == 0:
            model.eval()
            epoch_valid_loss = []
            with torch.no_grad():
                for x_val, y_val in tqdm(valid_loader, desc="Validation"):
                    x_val, y_val = x_val.to(device), y_val.to(device)
                    pred_val = model(x_val, 0, y_val) # y_val —É–∂–µ –Ω–∞ device
                    loss = loss_fn(x_val, pred_val)
                    epoch_valid_loss.append(loss.item())
            
            avg_valid_loss = sum(epoch_valid_loss) / len(epoch_valid_loss)
            history['valid_loss'].append(avg_valid_loss)
            print(f"Avg Validation Loss: {avg_valid_loss:.5f}")
            
            if lr_scheduler:
                if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    lr_scheduler.step(avg_valid_loss)
                else:
                    lr_scheduler.step()
            
            if avg_valid_loss < best_valid_loss:
                best_valid_loss = avg_valid_loss
                patience_counter = 0
                # –ú–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏ –ª—É—á—à—É—é –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                # best_model_state_on_valid = copy.deepcopy(model.state_dict())
                # torch.save(best_model_state_on_valid, os.path.join(checkpoint_path, "best_model_on_valid.pth"))
                print(f"‚ú® Validation loss improved to: {best_valid_loss:.5f}")
            elif early_stopping_patience:
                patience_counter += 1
                if patience_counter >= early_stopping_patience:
                    print(f"Stopping early. No improvement in validation loss for {patience_counter} epochs.")
                    if best_model_state_on_train:
                        model.load_state_dict(best_model_state_on_train)
                    return history

    # –í –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–∞–µ–º –≤ –º–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å –ª—É—á—à–∏–º train_loss
    print("Training finished. Loading the best model based on training loss.")
    if best_model_state_on_train:
        model.load_state_dict(best_model_state_on_train)
        
    return history
